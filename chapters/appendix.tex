\chapter{YOLOv3 network architecture} \label{appendix:A}
\section{Feature extractor}\label{feature_extractor}
YOLO don't use any of the already pre-trained backbone networks on image classification. Instead they trained their own classifier on the ImageNet data-set. The network uses successive $3 \times 3$ and $1 \times 1$ convolutional layers with some shortcut (skip) connections. It has 53 convolutional layers, therefore it has been named Darknet-53. Darknet is the deep learning framework used to train it. See \cref{table:Darknet-53}

Darknet-53 network is pretty good one compared to other backbones that are even deeper than that which makes it way faster at inference time. See \cref{table:backbone}.
Each network is trained with identical settings and tested at $256 \times 256$, single crop accuracy. Run times are measured on a Titan X at $256 \times 256$. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating
point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5$\times$ faster. Darknet-53 has similar performance to ResNet-152 and is 2$\times$ faster. Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That’s mostly because ResNets have just way too many layers and aren’t very efficient \cite{YOLOv3}.

\begin{table}[H]
  \centering
  \caption[Comparison of backbones]{Comparison of backbones. Accuracy, billions of operations, billion floating point operations per second, and FPS
  for various networks \cite{YOLOv3}.}\label{table:backbone}
  \begin{tabular}{@{}cccccc@{}}
    \toprule[1.5pt]
    \head{Backbone} & \head{Top-1} & \head{Top-5} & \head{Bn Ops} & \head{BFLOP/s} & FPS \\
    \midrule
    ResNet-101      & 77.1         & 93.7         & 19.7          & 1039           & 53 \\
    ResNet-152      & \textbf{77.6}         & \textbf{93.8}         & 29.4          & 1090           & 37 \\
    Darknet-53      & 77.2         & \textbf{93.8}         & 18.7          & \textbf{1457}           & \textbf{78} \\
    \bottomrule[1.5pt]
  \end{tabular}
\end{table}

\begin{table}[!htpb]
\centering
\caption{Darknet-53 \cite{YOLOv3}.}\label{table:Darknet-53}
\begin{tabular}{@{}ccccc@{}}
  \toprule[1.5pt]
            & \head{Type}       & \head{Filters}     & \head{Size}    & \head{Output} \\
  \midrule[1.5pt]
  &           Convolutional     & 32                 & $3 \times 3$   & $256 \times 256$ \\
  &           Convolutional     & 64                 & $3 \times 3 \ \text{/} 2$   & $128 \times 128$ \\
  \cmidrule{2-5}
  &           Convolutional     & 32                 & $1 \times 1$   &  \\
  1 $\times$&           Convolutional     & 64                 & $3 \times 3$   &  \\
  &           Residual          &                    &                & $128 \times 128$ \\
  \cmidrule{2-5}
  &           Convolutional     & 128                 & $3 \times 3 \ \text{/} 2$   & $64 \times 64$ \\
  \cmidrule{2-5}
  &           Convolutional     & 64                 & $1 \times 1$   &  \\
  2 $\times$&           Convolutional     & 128                 & $3 \times 3$   &  \\
  &           Residual          &                    &                & $64 \times 64$ \\
  \cmidrule{2-5}
  &           Convolutional     & 256                 & $3 \times 3 \ \text{/} 2$   & $32 \times 32$ \\
  \cmidrule{2-5}
  &           Convolutional     & 128                 & $1 \times 1$   &  \\
  8 $\times$&           Convolutional     & 256                 & $3 \times 3$   &  \\
  &           Residual          &                    &                & $32 \times 32$ \\
  \cmidrule{2-5}
  &           Convolutional     & 512                 & $3 \times 3 \ \text{/} 2$   & $16 \times 16$ \\
  \cmidrule{2-5}
  &           Convolutional     & 256                 & $1 \times 1$   &  \\
  8 $\times$&           Convolutional     & 512                 & $3 \times 3$   &  \\
  &           Residual          &                    &                & $16 \times 16$ \\
  \cmidrule{2-5}
  &           Convolutional     & 1024                & $3 \times 3 \ \text{/} 2$   & $8 \times 8$ \\
  \cmidrule{2-5}
  &           Convolutional     & 512                 & $1 \times 1$   &  \\
  4 $\times$&           Convolutional     & 1024                 & $3 \times 3$   &  \\
  &           Residual          &                    &                & $8 \times 8$ \\
  \cmidrule{2-5}
  &           Avgpool           &                    &Global      & \\
  &           Connected           &                    &1000      & \\
  &Softmax & & & \\
  \bottomrule[1.5pt]
\end{tabular}
\end{table}

\chapter[mAP for Object Detection]{mAP (mean Average Precision) for Object Detection}
AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. To understand it, its better to recap some related concepts.

\section{Precision and recall}
Precision measures how accurate is your predictions. i.e. the percentage of your predictions are correct.
Recall measures how good you find all the positives. For example, we can find 80\% of the possible positive cases in our top K predictions.
Here are their mathematical definitions:

$$
Precision = \frac{TP}{TP + FP}
$$
$$
Precision = \frac{TP}{TP + FN}
$$
$$
TP = True\ positive, \quad
FP = False\ positive, \quad
TN = True\ negative, \quad
FN = False\ negative.
$$

\section{Average Precision}
Let’s create an over-simplified example in demonstrating the calculation of the average precision. In this example, the whole dataset contains 5 apples only. We collect all the predictions made for apples in all the images and rank it in descending order according to the predicted confidence level. The second column indicates whether the prediction is correct or not. In this example, the prediction is correct if $IoU \geq 0.5$.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/map2.png}
	\caption{Example of precision and recall values.}\label{fig:map2}
\end{figure}

Let’s take the row with rank 3 and demonstrate how precision and recall are calculated first.
Precision is the proportion of TP = 2/3 = 0.67.
Recall is the proportion of TP out of the possible positives = 2/5 = 0.4.
Recall values increase as we go down the prediction ranking. However, precision has a zigzag pattern — it goes down with false positives and goes up again with true positives.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/map3.png}
	\caption{Plot of precision vs recall.}\label{fig:map3}
\end{figure}
The general definition for the Average Precision (AP) is finding the area under the precision-recall curve above.
$$
AP = \int_{0}^{1} p(r)dr
$$
Precision and recall are always between 0 and 1. Therefore, AP falls within 0 and 1 also. Before calculating AP for the object detection, we often smooth out the zigzag pattern first. Graphically, at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/map4.png}
	\caption{Elimination of zigzag pattern.}\label{fig:map4}
\end{figure}
