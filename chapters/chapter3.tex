\chapter{CNN application: Object detection}
The concept of convolution and convolutional neural networks has been applied to many real life problems: including object classification
object detection, speech recognition, disease depiction in medical images, self driving cars, and many more.
In this is chapter, we will focus on present the state-of-the-art detection systems: YOLO object detection which stands for you only look once
and R-CNN which stands for Region-CNN.
Object detection is the task of detecting, meaning classifying and localizing instances of semantic objects of a certain class (in our
case Algerian car plates along with their digits). An object detection algorithm should not only be able to classify an object but as well as
localizing it in an image by drawing a bounding box around it. See \cref{fig:detection}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/person.png}
  \caption{Example of what an object detection system should accomplish}\label{fig:detection}
\end{figure}

\section{YOLO: you only look once}
Over the past few years, the YOLO algorithm have evolved quite a lot going from YOLOv1 all through version four. The different
improvements that this algorithm went through are just the fruits of many research developments in the deep learning field incorporated into it to
make it more robust and less prone to errors. In this section we shall present the version three of YOLO. Version four has only been developed in
April 2020 during the middle of the pandemic. Many techniques have been included in this last paper which makes a bit difficult since we have to go
through all the new details. Therefore we shall only present version three which we already have a solid background of.

\subsection{Bounding boxes}
The YOLO algorithm divides the input image into an $S \times S$ grid. If the center of an object falls into a grid cell, that grid cell is responsible
for detecting that object \cite{YOLOv1}. Each grid cell predicts $B$ bounding boxes, using anchor boxes. Anchor boxes are predefined boxes of certain width and height.
Those boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect. They are typically chosen based on object
sizes in the training data \cite{MWAB}, see . Anchor boxes have been introduced to solve two issues (second issue discussed in \cref{sub:net_design}). Objects in the YOLO algorithm are associated with grid cells that their
centers fall into. If two objects' centers fall into the same grid cell we wont be able to predict both objects. Therefore, we can associate
each grid cell with multiple anchor boxes each responsible to detect only one object in that cell. A typical number of boxes used is three. See \cref{fig:yolo_output}.
Each bounding box is associated with a confidence score, which reflects how confident the network is that the bounding box contains an object (also called objectness) \cite{YOLOv1}.
This should be ideally 1 if there is an object otherwise 0 \cite{YOLOv3}.Then $b_{x},\ b_{y},\ b_{h},\ b_{w}$ that defines the bounding box, where $b_{x}$ and $b_{y}$ represents the box's
center coordinates and $b_{h},\ b_{w}$, the height and width respectively \cite{CERA}. And the class confidence scores. For instance if we are building a self driving car object detection system, we may want to detects cars, pedestrians and motorcycles. Therefore, each grid cell will be associated with
an $((5 + number\ of\ classes\ to\ detect) \times number\ of\ anchor\ boxes)$ dimensional vector. See \cref{fig:yolo_output}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/anchor_box.png}
  \caption[Example of anchor boxes]{Example of anchor boxes. As we can see on the figure, the anchor boxes capture the scale and aspect ratio of cars and pedestrians. Indeed, most cars and humans will have approximately the same scale and aspect ratio. The vector $y$ is composed of the objectness score as well as the bounding boxes and the class probabilities repeated for each anchor box. Here two anchor boxes have been used. YOLOv3 uses 3 anchor boxes. The image has been divided into a $3 \times 3$ grid just for illustration. The vector $y$ represents the manual labeling for the central cell. Anchor box 1 is associated with the pedestrian while the second one is associated with the car.}\label{fig:yolo_output}
\end{figure}


\subsection{Network design}\label{sub:net_design}\label{sec:network}

The network is a series of convolutional and pooling layers chosen so that the network eventually maps the input image $W \times H \times 3$ to
an output volume $S \times S \times ((5 + number\ of\ classes\ to\ detect) \times number\ of\ anchor\ boxes)$. YOLO's convolutional layers down-sample the image by a factor of 32
Now, to train the convolutional neural network, we pick an image size of $416 \times 416$.
This number has been chosen because we want an odd number of
locations in our feature map so there is a single center cell.
Objects, especially large objects, tend to occupy the center
of the image so it’s good to have a single location right at
the center to predict these objects instead of four locations
that are all nearby \cite{YOLOv2}.
so by using an input image of 416 we get an output feature map of $13 \times 13$.
The second issue anchor boxes address is the training instability \cite{YOLOv2}. In fact, during the early epochs of training if $b_{x}\ and\ b_{y}$ are randomly initialized, the network
struggles to converge to the right ground truth box's center. To overcome this problem, YOLO predicts location coordinates $b_{x}\ and\ b_{y}$ relative to the grid
cell. This bounds the ground truth to fall between 0 and 1. We use sigmoid activation to constrain the network's prediction to fall in this range.
The network predicts $B$ bounding boxes at each cell in the output feature map. The network predicts 5 coordinates for each bounding box $t_{x},\ t_{y},\ t_{h},\ t_{w}\ and\ t_{0}$, see \cref{fig:true_yolo_output}. If the cell is
offset from the top left corner of the image by $(c_{x}; c_{y})$ and
the anchor box has width and height $p{w},\ p_{h}$, then
the predictions correspond to \cite{YOLOv2}:

\begin{align}
  \label{bbox}
  b_{x} &= \sigma(t_{x}) + c_x \\
  b_{y} &= \sigma(t_{y}) + c_y \\
  b_{w} &= p_{w}e^{t_{w}} \\
  b_{h} &= p_{h}e^{t_{h}}
\end{align}

Since we constrain the location prediction the
parametrization is easier to learn, making the network
more stable \cite{YOLOv2}, see \cref{fig:bbox_calculation}.
The question that naturally rises is: How, at the beginning, do we get $p_{w},\ p_{h}$ ? Otherwise, how to assign an anchor box to a ground truth object ?
The answer to this question is given is \cref{IOU} as we need to define an important function (IoU) to proceed.

\bigskip
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/bbox.png}
  \caption[The true output of YOLOv3]{The true output of YOLOv3 after introducing the training instability issue. The net works output a $ 13 \times 13 \times (8 \times 3)$ in this case, or simply put $13 \times 13 \times 24$ output volume. Each grid cell outputs three bounding boxes.}\label{fig:true_yolo_output}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=.5]{figs/box.png}
  \caption[Bounding box calculation]{Bounding box calculation. We predict the width and height of the box as offsets from manually chosen anchor boxes. We predict the center
    coordinates of the box using a sigmoid function.}\label{fig:bbox_calculation}
\end{figure}

During training we optimize the following, multi-part loss function. As we can see the first sum is over scales, meaning different regions of the network. Indeed the network used in YOLOv3 does have only one output but three. The architecture of the network is discussed further in appendix.

\begin{multline}
  \label{equ:loss}
  \sum_{scales} \lambda_{coord} \sum_{i=0}^{S^2}  \sum_{j=0}^{B} {1}^{obj}_{i,j}  \big[ (t_x - \hat{t}_x)^2  +  (t_y - \hat{t}_y)^2  +  (t_w - \hat{t}_w)^2  +  (t_h - \hat{t}_h)^2  \big] \\
+ \sum_{i=0}^{S^2}  \sum_{j=0}^{B} {1}^{obj}_{i,j} \big[ - \log(\sigma(t_o))  + \sum_{k=1}^{C} BCE(\hat{y}_k, y_k \big]   \\
+ \lambda_{noobj} \sum_{i=0}^{S^2}  \sum_{j=0}^{B} {1}^{noobj}_{i,j}  \big[ -log(1-\sigma(t_o))  \big]
\end{multline}

where $1^{obi}_{i, j}$ denotes if object appears in cell $i$ and that the $j^{th}$ anchor box in cell $i$ is “responsible” for that prediction.
If an anchor box is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.
In cells that contain an object, the bounding box coordinates are calculated using the sum-squared loss function.
Each box predicts the classes the bounding box may contain using multi-label classification. In other words, binary cross-entropy loss is used (BCE).
The same binary cross-entropy loss is used to for objecness prediction as \cref{equ:loss} states it.

\subsection{Processing the algorithm's output}\label{IOU}

After training, the network at inference time will find multiple detections. In fact, for each cell in the $S \times S$ grid, using $B$ anchor
boxes, the algorithm will infer $B$ bounding boxes for each cell, which makes a total of $B \times S^2$. Therefore an object can be detected
multiple times.  \textbf{Non-max suppression} is an algorithm that cleans up those detections and makes sure each object gets detected only once.
Before discussing it though, let us introduce an important function called \textbf{Intersection over Union} (IoU for short) that calculates how much
a box or a rectangle overlaps another. So, IoU calculates the area defined by the intersection of the two boxes and divide it by the area defined by
their union. See \cref{fig:iou}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/iou_2.png}
  \caption{Intersection over union.}\label{fig:iou}
\end{figure}

IoU is an evaluation metric used to measure the accuracy of an object detection system on a particular data set. Indeed, most object detection
algorithm will judge a detection to be correct if the IoU between the ground truth box and the detected box is more than 0.5, see
\cref{fig:iou_sample}. We often see this evaluation metric used in object detection challenges such as the popular PASCAL VOC challenge \cite{pascal}. \\

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/sample_iou_scores.png}
  \caption{Sample IoU scores.}\label{fig:iou_sample}
\end{figure}

Back to our original question, how non-max suppression works. First, all the boxes having an $objectness \times the\ class\ probability$ less or
equal than some threshold are discarded (typical value used is .6). While there any remaining boxes, we pick the box with the largest $objectness
\times the\ class\ probability$ and output it as a prediction. Then we discard an remaining box with $IoU \geq 0.5$ with the box outputted in the
previous step. This algorithm ensures that each object is detected only once. \\

In \cref{sec:network} we discussed how the bounding boxes are being computed, and we finished it with a question: How does the anchor boxes being
assigned to ground truth objects at the beginning? YOLOv3 assigns the anchor with the highest Intersection-over-Union (IoU) overlap with a ground
truth box.

\section{Faster R-CNN}
Several object detection techniques and models have been developed over the years. Each with its benefits and drawbacks. In this section we shall explore the faster region-CNNs technique to tackle this task. \\

Faster R-CNN model is composed of two networks: region proposal network (RPN) for generating region proposals and a network using these proposals to detect objects. The main different here with its' predecessor Fast R-CNN is that the later uses an algorithm called "selective search" to generate region proposals. The time cost of generating region proposals is much smaller in RPN than selective search, since the RPN network does a significant part of computation which is overlapping  with the computation needed for the  object detection network. in short, RPN ranks region boxes (called anchors) from most likely to less likely to contain an object and proposes the ones most likely containing objects. The architecture is as shown in  \cref{fig:rcnn1}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/rcnn1.png}
	\caption{rcnn1}\label{fig:rcnn1}
\end{figure}

\subsection{Anchors}
In the default configuration of Faster R-CNN, it considers 9 anchors at each position of an image. \cref{fig:rcnn2} shows 9 anchors at the position $(320, 320)$ of an image with size $(600, 800)$. The colors represent three scales or sizes: $128 \times 128,\ 256 \times 256,\ 512 \times 512$. And in each color we have three boxes that have height width ratios $1:1,\ 1:2$ and $2:1$ respectively. These two parameters are called "scales" and "aspect ratios", and they have a significant effect on the performance of our model.
The RPN selects a position in a given image at every stride of 16 where it generates those 9 anchors. In an image of the same size as \cref{fig:rcnn2}  there will be 1989 $(39 \times 51)$ positions. This leads to 17901 $(1989 \times 9)$ boxes to consider. This number of anchors is hardly smaller than the technique of of sliding window and pyramid. The advantage here is that we can use region proposal network, to significantly reduce number of boxes that will be considered by the classifier network.

These anchors work well for Pascal VOC data-set as well as the COCO data-set. However you have the freedom to design different kinds of anchors/boxes. For example, you are designing a network to detect passengers/pedestrians, you may not need to consider the very short, very big, or square boxes. A uniform set of anchors may increase the speed as well as the accuracy.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/rcnn2.png}
	\caption{rcnn2}\label{fig:rcnn2}
\end{figure}

\subsection{Region Proposal Network}
The input to the RPN module is the feature map of an image, the RPN then generates centers on the original image for each "pixel" in a feature map obtained from a forward pass through a pre-trained CNN. It then generates 9 anchors around each center according to the specified scales and aspect ratios.
The output of a RPN is a set of probabilities  for each anchor that determine the probability of a certain anchor being an object or not. It also outputs a set of error estimations for the anchors which overlap with a ground truth box. these outputs will be examined by a classifier and regressor to eventually check the occurrence of objects. To be more precise, RPN predicts the possibility of an anchor being background or foreground, and refines the dimensions of an anchor.
Since the RPN performs a classification task, it will go through a training process for which we must have a clear definition of the data-set and the labels. In this case our data-set is the anchors defined for each image. As for the labels; the basic idea  is that we want to label the anchors having the higher overlaps with ground-truth boxes (the bounding box surrounding the object we wish to detect )as foreground, and the ones with lower overlaps as background. For this we use the IOU (Intersection Over Union) function. If the value of the IOU is higher than a certain threshold then it would be labeled as foreground otherwise it is labeled as background. \\

RPN also performs a regression task on the same anchors in order to correct the dimensions and location of these same anchors. For each anchor it computes an estimation of error on the dimensions called $t_{w}$ for the width and $t_{h}$ for the height as well as on the location of the anchor center $t_{x}$ and $t_{y}$ such that

\begin{align}
  t_{w} &= log(\frac{w}{w_{a}}) \\
  t_{h} &= log(\frac{h}{h_{a}}) \\
  t_{x} &= \frac{x - x_{a}}{w_{a}} \\
  t_{y} &= \frac{y - y_{a}}{h_{a}}
\end{align}

$x, y , w, h$ are the ground truth box center coordinates, width and height. $x_{a}, y_{a}, h_{a} and w_{a}$ and anchor boxes center coordinates, width and height.

The final and most important component of the training process is the loss function

\begin{equation}
  L(p_{i},t_{i}) = (\frac{1}{N_{cls}}) \sum_{i}^{} L_{cls}(p_{i},p_{i}^{*}) + \lambda (\frac{1}{N_{reg}}) \sum_{i}^{} p_{i}^{*} L_{reg}(t_{i},t_{i}^{*})
\end{equation}

where $p_{i}$ is the predicted probability of objectness and $p_{i}^*$ is the actual score. $t_{i}$ and $t_{i}^*$ are the predicted coordinates and actual coordinates respectively. The ground-truth label $p_{i}^*$ is 1 if the the anchor is positive and 0 if the anchor is negative.

\subsection{ROI Pooling}
Region of interest pooling (also known as RoI pooling) purpose is to perform max pooling on inputs of non-uniform sizes to obtain fixed-size feature maps (e.g. $7 \times 7$). This layer takes two inputs :

\begin{itemize}
	\item A fixed-size feature map obtained from a deep convolutional network with several convolutions and max-pooling layers
	\item An $N \times 5$ matrix of representing a list of regions of interest, where N is the number of RoIs. The first column represents the image index and the remaining four are the co-ordinates of the top left and bottom right corners of the region.
\end{itemize}

For every region of interest from the input list, it takes a section of the input feature map that corresponds to it and scales it to some pre-
defined size (e.g., $7 \times 7$). The scaling is done by:

\begin{itemize}
	\item Dividing the region proposal into equal-sized sections (the number of which is the same as the dimension of the output)
	\item Finding the largest value in each section
	\item Copying these max values to the output buffer.
\end{itemize}

The result is that from a list of rectangles with different sizes we can quickly get a list of corresponding feature maps with a fixed size. Note that the dimension of the RoI pooling output doesn’t actually depend on the size of the input feature map nor on the size of the region proposals. It’s determined solely by the number of sections we divide the proposal into. What’s the benefit of RoI pooling? One of them is processing speed. If there are multiple object proposals on the frame (and usually there’ll be a lot of them), we can still use the same input feature map for all of them. Since computing the convolutions at early stages of processing is very expensive, this approach can save us a lot of time. The \cref{fig:rcnn3} below shows the working of ROI pooling.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/rcnn3.png}
	\caption{rcnn3}\label{fig:rcnn3}
\end{figure}

This will be the input to a classifier network, which is a copy of the pre-trained backbone network we used to obtain the feature map, and which will be referred to as "Fast RCNN classifier network". This network will further branch out to a classification head and regression head. The loss function for the Fast-RCNN network is defined in the same way as the RPN loss, Except for the significance of the variables. $p_{i}$ is the predicted class scores for every class of objects we want to detect and $p_{i}^*$ is the actual score. $t_{i}$ and $t_{i}^*$ are the predicted cooridinates and actual coordinates, respectively. The ground-truth label $p_{i}^*$ is 1 for a certain class of objects if the region outputted by the ROI layer contains that object and 0 if it does not.

\subsection{Faster RCNN training}

For training the entire model there must be a well defined loss function which encapsulates all of the losses mentioned before. It can be considered as an estimation for error in the model, regardless of where the error occurs. The total loss is defined as nothing more than the sum of both losses (RPN loss and Fast RCNN classifier network loss).

\begin{equation}
  \text{Total loss = RPN loss + Fast RCNN classifier loss}
\end{equation}

The training process would proceed using the same optimization and regularization techniques discussed earlier.

%there is a last task that I have to do: talk about the network architecture of yolov3 in the appendix.
%add backbones
%add adam algorithm
%
